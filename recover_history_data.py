#!/usr/bin/env python3
"""
å†å²æ•°æ®æ¢å¤è„šæœ¬
æœç´¢å’Œæ¢å¤å¯èƒ½å­˜åœ¨çš„å†å²åˆ†ææ•°æ®
"""

import os
import json
import glob
from pathlib import Path
from datetime import datetime, timedelta
import shutil

def search_all_possible_locations():
    """æœç´¢æ‰€æœ‰å¯èƒ½çš„æ•°æ®å­˜å‚¨ä½ç½®"""
    print("ğŸ” æœç´¢æ‰€æœ‰å¯èƒ½çš„å†å²æ•°æ®...")
    
    search_locations = [
        "./data",
        "./results",  
        "./reports",
        "./cache",
        "./logs",
        "./web/data",
        "./tradingagents/data",
        "./temp",
        "./tmp",
        "~/.tradingagents",
        "/tmp/tradingagents"
    ]
    
    found_files = []
    
    for location in search_locations:
        expanded_path = os.path.expanduser(location)
        if os.path.exists(expanded_path):
            print(f"ğŸ“ æœç´¢ç›®å½•: {expanded_path}")
            
            # æœç´¢å„ç§å¯èƒ½çš„æ–‡ä»¶æ ¼å¼
            patterns = [
                "progress_*.json",
                "analysis_*.json", 
                "*.json",
                "*.md",
                "*analysis*",
                "*stock*",
                "*trading*"
            ]
            
            for pattern in patterns:
                try:
                    files = glob.glob(os.path.join(expanded_path, "**", pattern), recursive=True)
                    for file in files:
                        if is_analysis_file(file):
                            found_files.append(file)
                except Exception as e:
                    continue
    
    # å»é‡
    found_files = list(set(found_files))
    
    print(f"âœ… å…±æ‰¾åˆ° {len(found_files)} ä¸ªæ½œåœ¨çš„åˆ†ææ–‡ä»¶")
    return found_files


def is_analysis_file(file_path):
    """åˆ¤æ–­æ–‡ä»¶æ˜¯å¦ä¸ºåˆ†æç›¸å…³æ–‡ä»¶"""
    try:
        filename = os.path.basename(file_path).lower()
        
        # æ£€æŸ¥æ–‡ä»¶åç‰¹å¾
        analysis_keywords = [
            'progress', 'analysis', 'stock', 'trading', 
            'report', 'result', 'decision', 'market'
        ]
        
        if any(keyword in filename for keyword in analysis_keywords):
            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆé¿å…ç©ºæ–‡ä»¶æˆ–é…ç½®æ–‡ä»¶ï¼‰
            if os.path.getsize(file_path) > 100:  # å¤§äº100å­—èŠ‚
                return True
    except:
        pass
    return False


def analyze_file_content(file_path):
    """åˆ†ææ–‡ä»¶å†…å®¹æå–å…³é”®ä¿¡æ¯"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            if file_path.endswith('.json'):
                data = json.load(f)
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºåˆ†æè¿›åº¦æ–‡ä»¶
                if isinstance(data, dict):
                    analysis_info = {}
                    
                    # æå–åŸºæœ¬ä¿¡æ¯
                    analysis_info['file_path'] = file_path
                    analysis_info['file_size'] = os.path.getsize(file_path)
                    analysis_info['modification_time'] = datetime.fromtimestamp(
                        os.path.getmtime(file_path)
                    ).strftime('%Y-%m-%d %H:%M:%S')
                    
                    # æå–åˆ†æç›¸å…³ä¿¡æ¯
                    analysis_info['analysis_id'] = data.get('analysis_id', 'æœªçŸ¥')
                    analysis_info['status'] = data.get('status', 'æœªçŸ¥')
                    analysis_info['progress'] = data.get('progress_percentage', 0)
                    
                    # æå–è‚¡ç¥¨ä¿¡æ¯
                    stock_symbol = 'æœªçŸ¥'
                    raw_results = data.get('raw_results', {})
                    if isinstance(raw_results, dict):
                        stock_symbol = raw_results.get('stock_symbol', stock_symbol)
                    
                    # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•ä»analysis_idæå–
                    if stock_symbol == 'æœªçŸ¥' and 'analysis_id' in data:
                        import re
                        match = re.search(r'([A-Z]{1,5}|\d{6}|\d{3,4}\.HK)', 
                                        str(data['analysis_id']).upper())
                        if match:
                            stock_symbol = match.group(1)
                    
                    analysis_info['stock_symbol'] = stock_symbol
                    
                    # æå–æ—¶é—´ä¿¡æ¯
                    if 'start_time' in data:
                        analysis_info['start_time'] = datetime.fromtimestamp(
                            data['start_time']
                        ).strftime('%Y-%m-%d %H:%M:%S')
                    else:
                        analysis_info['start_time'] = 'æœªçŸ¥'
                    
                    return analysis_info
            
            else:
                # å¯¹äºéJSONæ–‡ä»¶ï¼Œå°è¯•æå–åŸºæœ¬ä¿¡æ¯
                content = f.read(1000)  # è¯»å–å‰1000å­—ç¬¦
                if any(keyword in content.lower() for keyword in 
                      ['stock', 'analysis', 'trading', 'investment']):
                    return {
                        'file_path': file_path,
                        'file_size': os.path.getsize(file_path),
                        'modification_time': datetime.fromtimestamp(
                            os.path.getmtime(file_path)
                        ).strftime('%Y-%m-%d %H:%M:%S'),
                        'file_type': 'text',
                        'preview': content[:200] + '...' if len(content) > 200 else content
                    }
    except Exception as e:
        return None
    
    return None


def recover_to_standard_location(analysis_files):
    """å°†æ‰¾åˆ°çš„åˆ†ææ–‡ä»¶æ¢å¤åˆ°æ ‡å‡†ä½ç½®"""
    print("\nğŸ”„ æ¢å¤æ•°æ®åˆ°æ ‡å‡†ä½ç½®...")
    
    # ç¡®ä¿æ ‡å‡†ç›®å½•å­˜åœ¨
    os.makedirs('./data/recovered', exist_ok=True)
    
    recovered_count = 0
    
    for file_info in analysis_files:
        try:
            source_path = file_info['file_path']
            filename = os.path.basename(source_path)
            
            # ç”Ÿæˆæ¢å¤åçš„æ–‡ä»¶å
            if file_info.get('analysis_id', 'æœªçŸ¥') != 'æœªçŸ¥':
                recovered_filename = f"recovered_{file_info['analysis_id']}.json"
            else:
                timestamp = file_info['modification_time'].replace(':', '-').replace(' ', '_')
                recovered_filename = f"recovered_{timestamp}_{filename}"
            
            dest_path = f"./data/recovered/{recovered_filename}"
            
            # å¤åˆ¶æ–‡ä»¶
            shutil.copy2(source_path, dest_path)
            print(f"âœ… æ¢å¤æ–‡ä»¶: {filename} -> {recovered_filename}")
            recovered_count += 1
            
        except Exception as e:
            print(f"âŒ æ¢å¤æ–‡ä»¶å¤±è´¥: {file_info['file_path']} - {e}")
    
    print(f"ğŸ‰ æˆåŠŸæ¢å¤ {recovered_count} ä¸ªæ–‡ä»¶åˆ° ./data/recovered/")


def generate_recovery_report(analysis_files):
    """ç”Ÿæˆæ•°æ®æ¢å¤æŠ¥å‘Š"""
    print("\nğŸ“Š ç”Ÿæˆæ•°æ®æ¢å¤æŠ¥å‘Š...")
    
    if not analysis_files:
        print("âŒ æœªæ‰¾åˆ°å¯æ¢å¤çš„æ•°æ®")
        return
    
    # æŒ‰æ—¥æœŸåˆ†ç»„
    by_date = {}
    for file_info in analysis_files:
        mod_time = file_info['modification_time']
        date_key = mod_time[:10]  # å–æ—¥æœŸéƒ¨åˆ†
        if date_key not in by_date:
            by_date[date_key] = []
        by_date[date_key].append(file_info)
    
    print(f"ğŸ“… æŒ‰æ—¥æœŸç»Ÿè®¡:")
    for date, files in sorted(by_date.items()):
        print(f"  {date}: {len(files)} ä¸ªæ–‡ä»¶")
        
        # æ˜¾ç¤ºè¯¥æ—¥æœŸçš„è‚¡ç¥¨
        stocks = set()
        for file_info in files:
            stock = file_info.get('stock_symbol', 'æœªçŸ¥')
            if stock != 'æœªçŸ¥':
                stocks.add(stock)
        
        if stocks:
            print(f"    è‚¡ç¥¨: {', '.join(sorted(stocks))}")
    
    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Šæ–‡ä»¶
    report_file = f"./data_recovery_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    try:
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump({
                'recovery_time': datetime.now().isoformat(),
                'total_files_found': len(analysis_files),
                'files_by_date': by_date,
                'detailed_files': analysis_files
            }, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    except Exception as e:
        print(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")


if __name__ == "__main__":
    print("ğŸ› ï¸ TradingAgents-CN å†å²æ•°æ®æ¢å¤å·¥å…·")
    print("=" * 60)
    
    # æœç´¢æ‰€æœ‰å¯èƒ½çš„æ–‡ä»¶
    found_files = search_all_possible_locations()
    
    if not found_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•å†å²æ•°æ®æ–‡ä»¶")
        print("\nğŸ’¡ å¯èƒ½çš„åŸå› :")
        print("1. æ•°æ®ç¡®å®å·²è¢«åˆ é™¤")
        print("2. æ•°æ®å­˜å‚¨åœ¨å…¶ä»–ä½ç½®")
        print("3. åˆ†æä»æœªæˆåŠŸå®Œæˆå¹¶ä¿å­˜")
        exit(1)
    
    # åˆ†ææ–‡ä»¶å†…å®¹
    print(f"\nğŸ“Š åˆ†æ {len(found_files)} ä¸ªæ–‡ä»¶...")
    analysis_files = []
    
    for file_path in found_files:
        file_info = analyze_file_content(file_path)
        if file_info:
            analysis_files.append(file_info)
    
    if not analysis_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„åˆ†ææ–‡ä»¶")
        exit(1)
    
    print(f"âœ… æ‰¾åˆ° {len(analysis_files)} ä¸ªæœ‰æ•ˆçš„åˆ†ææ–‡ä»¶")
    
    # æ˜¾ç¤ºæ‰¾åˆ°çš„æ–‡ä»¶
    print(f"\nğŸ“‹ æ‰¾åˆ°çš„åˆ†ææ–‡ä»¶:")
    for i, file_info in enumerate(analysis_files, 1):
        print(f"  {i}. è‚¡ç¥¨: {file_info.get('stock_symbol', 'æœªçŸ¥')}")
        print(f"     çŠ¶æ€: {file_info.get('status', 'æœªçŸ¥')}")
        print(f"     æ—¶é—´: {file_info.get('modification_time', 'æœªçŸ¥')}")
        print(f"     æ–‡ä»¶: {os.path.basename(file_info['file_path'])}")
        print()
    
    # æ¢å¤æ•°æ®
    recover_to_standard_location(analysis_files)
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_recovery_report(analysis_files)
    
    print("\n" + "=" * 60)
    print("ğŸ¯ æ•°æ®æ¢å¤å®Œæˆï¼")
    print("ğŸ’¡ å»ºè®®:")
    print("1. é‡å¯Webåº”ç”¨ä»¥åŠ è½½æ¢å¤çš„æ•°æ®")
    print("2. æ£€æŸ¥å†å²è®°å½•é¡µé¢æ˜¯å¦æ˜¾ç¤ºæ›´å¤šæ•°æ®")
    print("3. ä»Šååˆ†æä¼šè‡ªåŠ¨ä¿å­˜æ›´é•¿æ—¶é—´ï¼ˆ7å¤©ï¼‰")